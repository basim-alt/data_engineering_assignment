{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder. \\\n",
    "    appName(\"pyspark-1\"). \\\n",
    "    getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"/dataset/nyc-jobs.csv\", header=True)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_salary_frequency(df: DataFrame) -> list:\n",
    "    row_list = df.select('Salary Frequency').distinct().collect()\n",
    "    return [row['Salary Frequency'] for row in row_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of test function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mock_data = [('A', 'Annual'), ('B', 'Daily')]\n",
    "expected_result = ['Annual', 'Daily']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_get_salary_frequency(mock_data: list, \n",
    "                              expected_result: list,\n",
    "                              schema: list = ['id', 'Salary Frequency']):  \n",
    "    mock_df = spark.createDataFrame(data = mock_data, schema = schema)\n",
    "    assert get_salary_frequency(mock_df) == expected_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"NYC Jobs Analysis\").getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.listdir()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"/dataset/nyc-jobs.csv\", header=True, inferSchema=True)\n",
    "df.printSchema()\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, sum\n",
    "\n",
    "null_counts = df.select([sum(col(c).isNull().cast(\"int\")).alias(c) for c in df.columns])\n",
    "null_counts.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"salary_diff\", col(\"salary_to\") - col(\"salary_from\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import year, to_date\n",
    "\n",
    "df = df.withColumn(\"posting_date\", to_date(col(\"Posting Date\"), \"MM/dd/yyyy\"))\n",
    "\n",
    "df = df.withColumn(\"posting_year\", year(col(\"posting_date\"))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lower\n",
    "\n",
    "df = df.withColumn(\"requires_masters\", when(lower(col(\"Minimum Qual Requirements\")).contains(\"master\"), 1).otherwise(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_categories = df.groupBy(\"Job Category\").count().orderBy(col(\"count\").desc()).limit(10)\n",
    "top_categories.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "salary_by_category = df.groupBy(\"Job Category\").agg({\"avg_salary\": \"avg\"}).withColumnRenamed(\"avg(avg_salary)\", \"avg_salary\").orderBy(col(\"avg_salary\").desc())\n",
    "\n",
    "salary_by_category.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "highest_salary_agency = df.groupBy(\"Agency\").agg({\"avg_salary\": \"max\"}).withColumnRenamed(\"max(avg_salary)\", \"max_salary\").orderBy(col(\"max_salary\").desc())\n",
    "\n",
    "highest_salary_agency.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recent_salary = df.filter(col(\"posting_year\") >= 2024).groupBy(\"Agency\").agg({\"avg_salary\": \"avg\"}).withColumnRenamed(\"avg(avg_salary)\", \"avg_salary_last_2_years\")\n",
    "\n",
    "recent_salary.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumnRenamed(\"posting date\", \"posting_date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(\"posting date\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(\"posting_date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"/dataset/nyc-jobs.csv\", header=True, inferSchema=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.toDF(*[c.strip().lower().replace(\" \", \"_\")for c in df.columns])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_timestamp, year, col\n",
    "\n",
    "df = df.withColumn(\n",
    "    \"posting_timestamp\",\n",
    "    to_timestamp(col(\"posting_date\"), \"yyyy-MM-dd'T'HH:mm:ss.SSS\")\n",
    ")\n",
    "\n",
    "df = df.withColumn(\n",
    "    \"posting_year\",\n",
    "    year(col(\"posting_timestamp\"))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(\"posting_year\").distinct().orderBy(\"posting_year\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_categories = df.groupBy(\"Job_Category\") \\\n",
    "    .count() \\\n",
    "    .orderBy(col(\"count\").desc()) \\\n",
    "    .limit(10)\n",
    "\n",
    "top_categories.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "df = df.withColumn(\n",
    "    \"avg_salary\",\n",
    "    (col(\"salary_range_from\") + col(\"salary_range_to\")) / 2\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "salary_by_category = df.groupBy(\"Job_Category\").agg({\"avg_salary\": \"avg\"}).withColumnRenamed(\"avg(avg_salary)\", \"avg_salary\") \\\n",
    "    .orderBy(col(\"avg_salary\").desc())\n",
    "salary_by_category.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_categories = df.groupBy(\"Job_Category\") \\\n",
    "    .count() \\\n",
    "    .orderBy(col(\"count\").desc()) \\\n",
    "    .limit(10)\n",
    "\n",
    "top_categories.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "salary_by_category = df.groupBy(\"Job_Category\") \\\n",
    "    .agg({\"avg_salary\": \"avg\"}) \\\n",
    "    .withColumnRenamed(\"avg(avg_salary)\", \"avg_salary\") \\\n",
    "    .orderBy(col(\"avg_salary\").desc())\n",
    "\n",
    "salary_by_category.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "highest_salary_agency = df.groupBy(\"Agency\") \\\n",
    "    .agg({\"avg_salary\": \"max\"}) \\\n",
    "    .withColumnRenamed(\"max(avg_salary)\", \"max_salary\") \\\n",
    "    .orderBy(col(\"max_salary\").desc())\n",
    "\n",
    "highest_salary_agency.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recent_salary = df.filter(col(\"posting_year\") >= 2024) \\\n",
    "    .groupBy(\"Agency\") \\\n",
    "    .agg({\"avg_salary\": \"avg\"}) \\\n",
    "    .withColumnRenamed(\"avg(avg_salary)\", \"avg_salary_last_2_years\")\n",
    "\n",
    "recent_salary.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import max\n",
    "\n",
    "max_year = df.select(max(\"posting_year\")).collect()[0][0]\n",
    "\n",
    "recent_salary = df.filter(col(\"posting_year\") >= (max_year - 1)).groupBy(\"Agency\").agg({\"avg_salary\": \"avg\"}).withColumnRenamed(\"avg(avg_salary)\", \"avg_salary_last_2_years\")\n",
    "\n",
    "recent_salary.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode, split\n",
    "\n",
    "skills_df = df.withColumn(\n",
    "    \"skill\",\n",
    "    explode(split(col(\"Preferred_Skills\"), \",\"))\n",
    ")\n",
    "\n",
    "highest_skills = skills_df.groupBy(\"skill\").agg({\"avg_salary\": \"avg\"}).withColumnRenamed(\"avg(avg_salary)\", \"avg_salary\").orderBy(col(\"avg_salary\").desc())\n",
    "\n",
    "highest_skills.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "top_cat_pd = top_categories.toPandas()\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.bar(top_cat_pd[\"Job_Category\"], top_cat_pd[\"count\"])\n",
    "plt.xticks(rotation=90)\n",
    "plt.title(\"Top 10 Job Categories\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.mode(\"overwrite\") \\\n",
    "    .option(\"header\", True) \\\n",
    "    .csv(\"output/processed_nyc_jobs.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
